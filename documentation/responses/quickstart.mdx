---
title: 'Quickstart'
description: 'Get started with the Julep Responses API for LLM interactions'
icon: 'rocket'
---

## Introduction

The Julep Responses API provides a lightweight interface for generating content with Large Language Models (LLMs) without needing to create persistent agents or sessions. 
It's perfect for quick, one-off interactions like content generation, text processing, or question answering.

Unlike OpenAI's Responses API which is limited to their models, Julep's implementation serves as an open-source compatibility layer that allows you to use models from any provider, eliminating vendor lock-in and providing flexibility in AI product development. Being designed to be compatible with OpenAI's interface, it not only makes it easier to migrate existing applications that use OpenAI's interface, but also allows you to leverage model diversity, reduce dependency risks, future-proof your applications, and optimize costs.

- **Leverage model diversity**: Choose the best models for specific tasks based on performance, cost, or specialized capabilities
- **Reduce dependency risks**: Avoid being affected by outages or pricing changes from a single provider
- **Future-proof your applications**: Easily adopt new models as they emerge without rewriting your integration code
- **Optimize costs**: Select models based on the best price-performance ratio for each use case

To conclude, the Julep Responses API provides a consistent experience across all supported models with standardized parameter handling and response formatting, making it easier to experiment with different models while maintaining application stability.

<Warning>
- Please note that the Julep Responses API is not hosted by Julep. You need to host your own instance of the Julep Responses API to use it. See the [installation guide](/responses/quickstart/#local-installation) for details.
- Being in beta, the Julep Responses API is subject to change. Please check back frequently for the latest updates on feature availability.
- To learn more about the motivations and design principles behind the Julep Responses API, check out the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) documentation.
</Warning>

## Local Installation

This section will guide you through the steps to set up the Responses API using Docker.

<Steps>

<Step title="Prerequisites">

Install [Docker](https://docs.docker.com/get-docker/)

</Step>

<Step title="Create a directory for the project">

```bash
mkdir julep-responses-api
```

</Step>

<Step title="Navigate to the project directory">

```bash
cd julep-responses-api
```

</Step>

<Step title="Download and edit the environment variables">

```bash
wget https://u.julep.ai/responses-env.example -O .env
```

Edit the `.env` file with your own values.

</Step>

<Step title="Download the Docker Compose file">

```bash
wget https://u.julep.ai/responses-compose.yaml
```
Download the file to the current directory with the name `responses-compose.yaml`. This is the file that will be used to run the Docker containers.

</Step>

<Step title="Run the Docker containers">

```bash
docker compose -f responses-compose.yaml up --watch
```

This will start the containers in watch mode.

</Step>

<Step title="Verify that the containers are running">

```bash
docker ps
```

</Step>

</Steps>

## Quickstart Example

With the OpenAI client initialized, you can now use the Responses API to generate content.

<Note>
- RESPONSE_API_KEY is the API key that you set in the `.env` file.
- While using the models other than OpenAI, please you might need to add the `provider/` prefix to the model name. To learn more about the supported providers, check out the [LiteLLM Providers](https://docs.litellm.ai/docs/providers) page.
- Make sure to add the relvant Provider keys to the `.env` file as well to use the models from the supported providers.
</Note>

### 1. Install the OpenAI SDK

<CodeGroup>
```bash pip
pip install openai
```

```bash npm
npm install openai
```
</CodeGroup>

### 2. Initialize the OpenAI client

<CodeGroup>

```python Python
from openai import OpenAI
openai_client = OpenAI(base_url="http://localhost:8080/", api_key="RESPONSE_API_KEY")
```

```javascript Node.js
import { OpenAI } from 'openai';
const openai_client = new OpenAI({ baseURL: 'http://localhost:8080/', apiKey: 'RESPONSE_API_KEY' });
```

</CodeGroup>

### 3. Generate a response

<CodeGroup>

```python Python
import os
from openai import OpenAI

openai_client = OpenAI(base_url="http://localhost:8080/", api_key=os.getenv("RESPONSE_API_KEY"))

response = openai_client.responses.create(
    model="gpt-4o-mini",
    input="How many people live in the world?"
)
print("Generated response:", response.output[0].content[0].text)
```

```javascript Node.js

import { OpenAI } from 'openai';

const openai_client = new OpenAI({ baseURL: 'http://localhost:8080/', apiKey: "RESPONSE_API_KEY" });

const response = await openai_client.responses.create({
    model: "gpt-4o-mini",
    input: "How many people live in the world?"
});

console.log("Generated response:", response.output[0].content[0].text);
```

</CodeGroup>


## Next Steps

Now that you've seen the quickstart example, you can learn more about the Responses API by:
- Exploring the [Concepts](/responses/concepts) page to understand the core components
- Following the [Tutorials](/responses/tutorials) for more detailed code examples and use cases