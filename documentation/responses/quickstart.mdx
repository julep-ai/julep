---
title: 'Quickstart'
description: 'Get started with Julep Open Responses API for LLM interactions'
icon: 'rocket'
---

## Introduction

Julep's Open Responses is a self-hosted, open-source implementation of OpenAI's Responses API that works with any LLM backend. It provides a lightweight interface for generating content with Large Language Models (LLMs) without needing to create persistent agents or sessions.

### What is Open Responses?

Julep's Open Responses lets you run your own server that is compatible with OpenAI's Responses API, while giving you the freedom to use alternative models like:
- Anthropic's Claude
- Alibaba's Qwen
- Deepseek R1
- and many others ...

It's essentially a drop-in replacement that you control, with a permissive Apache-2.0 license. As an early release, we welcome your feedback and contributions to help improve it.

<Frame caption="Open Responses API Overview"> 
  <img
    src="/images/open-responses.jpg"
    alt="Open Responses API Diagram"
  />
</Frame>

### Why Open Responses?

- **Model Flexibility**: Use any LLM backend without vendor lock-in, including local model deployment
- **Self-hosted & Private**: Maintain full control over your deployment on your own infrastructure (cloud or on-premise)
- **Drop-in Compatibility**: Seamlessly integrates with the official Agents SDK by simply pointing to your self-hosted URL
- **Easy Deployment**: Quick setup via docker-compose or our CLI with minimal configuration
- **Built-in Tools**: Automatic execution of tool calls (like web_search) using open & pluggable alternatives

<Warning>
- The Open Responses API requires self-hosting. See the [installation guide](#local-installation) below.
- Being in Alpha, the API is subject to change. Check back frequently for updates.
- For more context, see the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) documentation.
</Warning>

## Local Installation

This section will guide you through the steps to set up the Julep's Open Responses API.

### Prerequisites

Install [Docker](https://docs.docker.com/get-docker/)

### Installation

The Julep's Open Responses API is a fully microservice-based architecture. It is fully dockerized and can be easily deployed on any infrastructure that supports Docker. There are two ways to install the API:

- [Docker Installation](#docker-installation)
- [CLI Installation](#cli-installation)

#### Docker Installation

<Steps>

<Step title="Create a directory for the project">

```bash
mkdir julep-responses-api
```

</Step>

<Step title="Navigate to the project directory">

```bash
cd julep-responses-api
```

</Step>

<Step title="Download and edit the environment variables">

```bash
wget https://u.julep.ai/responses-env.example -O .env
```

Edit the `.env` file with your own values.

</Step>

<Step title="Download the Docker Compose file">

```bash
wget https://u.julep.ai/responses-compose.yaml -O docker-compose.yml
```
Download the file to the current directory with the name `docker-compose.yml`. This is the file that will be used to run the Docker containers.

</Step>

<Step title="Run the Docker containers">

```bash
docker compose up --watch
```

This will start the containers in watch mode.

</Step>

<Step title="Verify that the containers are running">

```bash
docker ps
```

</Step>

</Steps>

#### CLI Installation

The CLI is a lightweight alternative to Docker for those who prefer not to use Docker directly. 

<Info>
Internally, it uses Docker to run the containers.
</Info>

<Steps>

<Step title="Download the CLI">
<CodeGroup>
```bash npx
npx install open-responses
```

```bash uvx
uvx install open-responses
```

</CodeGroup>

</Step>

<Step title="Setup the Environment Variables">

<CodeGroup>
```bash npx
npx open-responses setup
```

```bash uvx
uvx open-responses setup
```
</CodeGroup>

</Step>

<Step title="Run the CLI">

<CodeGroup>
```bash npx
npx open-responses up
```

```bash uvx
uvx open-responses up
```
</CodeGroup>
</Step>

</Steps>

<Info>
To learn more about the CLI one can use the `npx open-responses --help` or `uvx open-responses --help` command.
</Info>

## Quickstart Example

With the OpenAI SDK initialized, you can now use the Responses API to generate content.

<Note>
  <h4>API Key Configuration</h4>
  - `RESPONSE_API_KEY` is the API key that you set in the `.env` file.
  
  <h4>Model Selection</h4>
  - While using models other than OpenAI, one might need to add the `provider/` prefix to the model name.
  - For supported providers, see the [LiteLLM Providers](https://docs.litellm.ai/docs/providers) documentation.
  
  <h4>Environment Setup</h4>
  - Add the relevant provider keys to the `.env` file to use their respective models.
</Note>

### 1. Install the OpenAI SDK

<CodeGroup>
```bash pip
pip install openai
```

```bash npm
npm install openai
```
</CodeGroup>

### 2. Initialize the OpenAI client

<CodeGroup>

```python Python
from openai import OpenAI
openai_client = OpenAI(base_url="http://localhost:8080/", api_key="RESPONSE_API_KEY")
```

```javascript Node.js
import { OpenAI } from 'openai';
const openai_client = new OpenAI({ baseURL: 'http://localhost:8080/', apiKey: 'RESPONSE_API_KEY' });
```

</CodeGroup>

### 3. Generate a response

<CodeGroup>

```python Python
import os
from openai import OpenAI

openai_client = OpenAI(base_url="http://localhost:8080/", api_key=os.getenv("RESPONSE_API_KEY"))

response = openai_client.responses.create(
    model="gpt-4o-mini",
    input="How many people live in the world?"
)
print("Generated response:", response.output[0].content[0].text)
```

```javascript Node.js

import { OpenAI } from 'openai';

const openai_client = new OpenAI({ baseURL: 'http://localhost:8080/', apiKey: "RESPONSE_API_KEY" });

const response = await openai_client.responses.create({
    model: "gpt-4o-mini",
    input: "How many people live in the world?"
});

console.log("Generated response:", response.output[0].content[0].text);
```

</CodeGroup>


## Next Steps

You've got Open Responses running – here's what to explore next:

- [Learn more about the Open Responses API Concepts](/responses/concepts) – To learn more about core concepts and how Open Responses structures these building blocks in your applications.
- [Learn more about the Open Responses API Tutorials](/responses/tutorials) – To follow step-by-step guides to build your first agent and integrate tools.
- [Learn more about the Open Responses API Roadmap](/responses/roadmap) – To see upcoming features including:
- [Learn more about Julep](/docs/introduction/julep) - To learn more about Julep and its features
- [GitHub](https://github.com/julep-ai/julep) - To contribute to the project