---
title: 'Responses API Concepts'
description: 'Understanding Julep Responses API concepts'
icon: 'comment-dots'
---

## 1. Overview

In this section, we'll cover the key concepts and components of the Responses API. The Responses API is designed to be compatible with OpenAI's interface, making it easy to migrate existing applications that use OpenAI's API to Julep.

<Tip>
The Responses API is designed to be compatible with OpenAI's interface, making it easy to migrate existing applications that use OpenAI's API to Julep.
</Tip>
<Note>
The Responses API is currently in beta and subject to change. To learn more about the motivations and design principles behind the Responses API, check out the [ OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) documentation.
</Note>
<Warning>
Julep doesn't host the Responses API. You need to host your own instance of the Responses API to use it. The instructions to host your own instance are [here](/responses/quickstart/#docker-setup).
</Warning>

## 2. Components

The Responses API offers a streamlined way to interact with language models with the following key components:

- **Response ID**: A unique identifier (`uuid7`) for each response.
- **Model**: The language model used to generate the response (e.g., "claude-3.5-haiku", "gpt-4o", etc.).
- **Input**: The prompt or question sent to the model, which can be simple text or structured input.
- **Output**: The generated content from the model, which can include text, tool outputs, or other structured data.
- **Status**: The current status of the response (completed, failed, in_progress, incomplete).
- **Tools**: Optional tools that the model can use to enhance its response.
- **Usage**: Token consumption metrics for the response.

### 2.1. Response Configuration Options

When creating a response, you can leverage these configuration options to tailor the experience:

| Option                | Type                                     | Description                                                                                       | Default        | Status        |
|-----------------------|------------------------------------------|---------------------------------------------------------------------------------------------------|----------------|---------------|
| `model`               | `string`                                 | The language model to use (e.g., "claude-3.5-haiku", "gpt-4o"). Check out the [supported models](/docs/integrations/supported-models) for more information. | Required       | Implemented   |
| `input`               | `string` \| `array`                      | The prompt or structured input to send to the model                                               | Required       | Implemented   |
| `include`             | `array` \| `null`                        | Types of content to include in the response (e.g., "file_search_call.results")                    | `None`         | Partially Implemented |
| `parallel_tool_calls` | `boolean`                                | Whether to allow tools to be called in parallel                                                   | `true`         | Implemented   |
| `store`               | `boolean`                                | Whether to store the response for later retrieval                                                 | `true`         | Implemented   |
| `stream`              | `boolean`                                | Whether to stream the response as it's generated                                                  | `false`        | Planned       |
| `max_tokens`          | `integer` \| `null`                      | Maximum number of tokens to generate                                                              | `None`         | Implemented   |
| `temperature`         | `number`                                 | Controls randomness in response generation (0 to 1)                                               | `1`            | Implemented   |
| `top_p`               | `number`                                 | Controls diversity in token selection (0 to 1)                                                    | `1`            | Implemented   |
| `n`                   | `integer` \| `null`                      | Number of responses to generate                                                                   | `None`         | Implemented   |
| `stop`                | `string` \| `array` \| `null`            | Sequence(s) where the model should stop generating                                                | `None`         | Implemented   |
| `presence_penalty`    | `number` \| `null`                       | Penalty for new tokens based on presence in text so far                                           | `None`         | Implemented   |
| `frequency_penalty`   | `number` \| `null`                       | Penalty for new tokens based on frequency in text so far                                          | `None`         | Implemented   |
| `logit_bias`          | `object` \| `null`                       | Modify likelihood of specific tokens appearing                                                    | `None`         | Implemented   |
| `user`                | `string` \| `null`                       | Unique identifier for the end-user                                                                | `None`         | Implemented   |
| `instructions`        | `string` \| `null`                       | Additional instructions to guide the model's response                                             | `None`         | Implemented   |
| `previous_response_id`| `string` \| `null`                       | ID of a previous response for context continuity                                                  | `None`         | Implemented   |
| `reasoning`           | `object` \| `null`                       | Controls reasoning effort (low/medium/high)                                                       | `None`         | Implemented   |
| `text`                | `object` \| `null`                       | Configures text format (text or JSON object)                                                      | `None`         | Implemented   |
| `tool_choice`         | `"auto"` \| `"none"` \| `object` \| `null` | Controls how the model chooses which tools to use                                               | `None`         | Implemented   |
| `tools`               | `array` \| `null`                        | List of tools the model can use for generating the response                                       | `None`         | Partially Implemented |
| `truncation`          | `"disabled"` \| `"auto"` \| `null`       | How to handle context overflow                                                                    | `None`         | Planned       |
| `metadata`            | `object` \| `null`                       | Additional metadata for the response                                                              | `None`         | Implemented   |


## 3. Implementation Status

The Responses API is under active development. Here's the current implementation status:

### 3.1. Implemented Features âœ…

- Basic response creation and retrieval
- Text and image input support
- Function calling
- Web search tool support
- Response metadata
- Reasoning support
- Tool choice configuration
- Multiple input and output formats

### 3.2. Partially Implemented Features ðŸ”„

- File search tool (in progress)
- Reasoning effort (mapping to thinking budget)
- Annotations for citations

### 3.3. Planned Features ðŸ”œ

- Streaming support
- Response deletion endpoint
- Computer tool call support (compatibility with OpenAI)
- Truncation auto support
- GET /responses/{response_id}/input_items endpoint
- Pagination for input items

## 4. Input Formats

The Responses API supports various input formats to accommodate different use cases:

### 4.1. Simple Text Input

The simplest way to interact with the Responses API is to provide a text string as input:

<CodeGroup>
```json Simple Text Input
{
  "input": "What are the top 5 skincare products?"
}
```
</CodeGroup>

### 4.2. Structured Message Input

For more complex interactions, you can provide a structured array of messages:

<CodeGroup>
```json Structured Message Input
{
  "input": [
    {
      "role": "user",
      "content": "Please summarize the current market trends in renewable energy."
    }
  ]
}
```
</CodeGroup>

### 4.3. Multi-modal Input

The Responses API supports multi-modal inputs, allowing you to include images or files along with text:

<CodeGroup>
```json Multi-modal Input
{
  "input": [
    {
      "role": "user",
      "content": [
          {"type": "input_text", "text": "what is in this image?"},
          {
              "type": "input_image",
              "image_url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
          }
      ]
    }
  ]
}
```
</CodeGroup>

## 5. Tool Usage

The Responses API supports tool usage, allowing the model to perform actions like web searches, function calls, and more to enhance its response.

<CodeGroup>
```json Web Search Tool Definition
{
  "input": "What are the latest advancements in quantum computing?",
  "tools": [
    {
      "type": "web_search_preview",
      "domains": ["https://www.google.com"],
      "search_context_size": "small",
      "user_location": { 
        "type": "approximate",
        "city": "YOUR_CITY",
        "country": "YOUR_COUNTRY",
        "region": "YOUR_REGION",
        "timezone": "YOUR_TIMEZONE"
      }
    }
  ],
}
```

```json Sample Function Tool Call to get the weather
{
  "input": "What's the weather in San Francisco?",
  "tools": [
    {
      "type": "function",
      "name": "get_weather",
      "description": "Get the current weather in a location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA"
          },
          "unit": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "The unit of temperature to use"
          }
        },
        "required": ["location"]
      }
    }
  ],
}
```
</CodeGroup>

## 6. How to Use Responses

The Responses API provides a straightforward way to interact with language models for quick, one-off interactions. Here's how to use it with OpenAI's SDKs:

### 6.1. Creating a Response

<CodeGroup>
```python Python
from openai import OpenAI

openai_client = OpenAI(base_url="http://localhost:8080/api", api_key="YOUR_JULEP_API_KEY")
response = openai_client.responses.create(
    model="claude-3.5-haiku",
    input="Think about this math problem and try to solve it: If a train leaves Chicago at 2pm traveling west at 60mph, and another train leaves Denver at 3pm traveling east at 75mph, and the distance between Chicago and Denver is 1000 miles, at what time will the trains meet?",
    tools=None, # Optional, defaults to None
)

print("Generated response:", response.output[0].content[0].text)
```

```javascript Node.js
import { OpenAI } from 'openai';

const openai_client = new OpenAI({ baseURL: 'http://localhost:8080/api', apiKey: 'YOUR_JULEP_API_KEY' });
const response = await openai_client.responses.create({
    model: "claude-3.5-haiku",
    input: "Think about this math problem and try to solve it: If a train leaves Chicago at 2pm traveling west at 60mph, and another train leaves Denver at 3pm traveling east at 75mph, and the distance between Chicago and Denver is 1000 miles, at what time will the trains meet?",
    tools: null, // Optional, defaults to None
});

console.log("Generated response:", response.output[0].content[0].text);
```
</CodeGroup>

Follow the example below to see a sample response from the Responses API.

<AccordionGroup>
<Accordion title="Sample Response">

```json JSON Response
{'id': '067dca81-8c8a-7f8a-8000-5fd0beb5ef60',
 'created_at': 1742514226.0,
 'error': None,
 'incomplete_details': None,
 'instructions': None,
 'metadata': {},
 'model': 'o1',
 'object': 'response',
 'output': [{'id': '067dca83-29fe-7a04-8000-ef2ef8ca940e',
   'content': [{'annotations': [],
     'text': 'First, note that from 2 p.m. to 3 p.m., the train leaving Chicago travels for 1 hour at 60 mph, covering 60 miles. After this hour, the remaining distance between the two trains is 1000 âˆ’ 60 = 940 miles.\n\nStarting at 3 p.m., the two trains approach each other. Their combined speed is 60 + 75 = 135 mph. Let T be the number of hours after 3 p.m. until they meet. Then:\n\n135 Ã— T = 940  \nT = 940 Ã· 135  \nT â‰ˆ 6.96 hours\n\nThis is about 6 hours and 57 minutes, so starting from 3 p.m., they meet at approximately 9:57 p.m.',
     'type': 'output_text'}],
   'role': 'assistant',
   'status': 'completed',
   'type': 'message'}],
 'parallel_tool_calls': True,
 'temperature': 1.0,
 'tool_choice': 'auto',
 'tools': [],
 'top_p': 1.0,
 'max_output_tokens': None,
 'previous_response_id': None,
 'reasoning': {'effort': 'medium', 'generate_summary': None},
 'status': 'completed',
 'text': {'format': None, 'type': 'text'},
 'truncation': 'auto',
 'usage': {'input_tokens': 80,
  'output_tokens': 648,
  'output_tokens_details': {'reasoning_tokens': 0},
  'total_tokens': 728,
  'input_tokens_details': {'cached_tokens': 0}},
 'user': '00000000-0000-0000-0000-000000000000',
 'store': True}
```
</Accordion>
</AccordionGroup>

#### 6.1.1. Using Function Calling Tools

<CodeGroup>
```python Python
response = client.responses.create(
    model="gpt-4o",
    input="What's the weather in New York City?",
    tools=[{
        "type": "function",
        "name": "get_weather",
        "description": "Get the current weather in a location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "The city and state, e.g. New York, NY"
                },
                "unit": {
                    "type": "string",
                    "enum": ["celsius", "fahrenheit"],
                    "description": "The unit of temperature to use"
                }
            },
            "required": ["location"]
        }
    }]
)

# The output will include both the text response and any tool calls that were made
```

```javascript Node.js
const response = await client.responses.create({
    model: "gpt-4o",
    input: "What's the weather in New York City?",
    tools: [{
        type: "function",
        name: "get_weather",
        description: "Get the current weather in a location",
        parameters: {
            type: "object",
            properties: {
                location: {
                    type: "string",
                    description: "The city and state, e.g. New York, NY"
                },
                unit: {
                    type: "string",
                    enum: ["celsius", "fahrenheit"],
                    description: "The unit of temperature to use"
                }
            },
            required: ["location"]
        }
    }]
});

// The output will include both the text response and any tool calls that were made
```
</CodeGroup>

Follow the example below to see a sample response from the Responses API.

<AccordionGroup>
<Accordion title="Sample Response">

```bash Sample Response
Response(id='067dca14-f985-7b38-8000-dbc22a251646', created_at=1742512464.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o', object='response', output=[ResponseFunctionToolCall(id='call_djdaM8p0VI1AdHiUSdNwGN8l', arguments='{"location":"Paris, France","unit":"celsius"}', call_id='call_djdaM8p0VI1AdHiUSdNwGN8l', name='get_current_weather', type='function_call', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=None, status='completed', text=ResponseTextConfig(format=None, type='text'), truncation='auto', usage=ResponseUsage(input_tokens=88, output_tokens=23, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=111, input_tokens_details={'cached_tokens': 0}), user='00000000-0000-0000-0000-000000000000', store=True)
```
</Accordion>
</AccordionGroup>

#### 6.1.2. Using Web Search Tool

<CodeGroup>
```python Python
from openai import OpenAI

openai_client = OpenAI(base_url="http://localhost:8080/api", api_key="YOUR_JULEP_API_KEY")

response = openai_client.responses.create(
    model="gpt-4o",
    tools=[{"type": "web_search_preview"}],
    input="What was a positive news story from today?",
)
```

```javascript Node.js
import { OpenAI } from 'openai';

const openai_client = new OpenAI({ baseURL: 'http://localhost:8080/api', apiKey: 'YOUR_JULEP_API_KEY' });
const response = await openai_client.responses.create({
    model: "gpt-4o",
    tools: [{type: "web_search_preview"}],
    input: "What was a positive news story from today?",
});
```
</CodeGroup>

Follow the example below to see a sample response from the Responses API.

<AccordionGroup>
<Accordion title="Sample Response">

```json JSON Response
{'id': '067dcaec-e692-7fdb-8000-1a1cc88b7f2c',
 'created_at': 1742515935.0,
 'error': None,
 'incomplete_details': None,
 'instructions': None,
 'metadata': {},
 'model': 'gpt-4o',
 'object': 'response',
 'output': [{'id': 'call_egAEiEneIwGp3BvtEZzSxdpD',
   'status': 'completed',
   'type': 'web_search_call'},
  {'id': '067dcaed-f2c4-71d3-8000-b7ebdf69f8ba',
   'content': [{'annotations': [],
     'text': 'To find a positive news story from today, you can check reliable sources such as [Good News Network](https://www.goodnewsnetwork.org/) or [Positive News](https://www.positive.news/). These platforms focus on uplifting and inspiring stories from around the world.',
     'type': 'output_text'}],
   'role': 'assistant',
   'status': 'completed',
   'type': 'message'}],
 'parallel_tool_calls': True,
 'temperature': 1.0,
 'tool_choice': 'auto',
 'tools': [],
 'top_p': 1.0,
 'max_output_tokens': None,
 'previous_response_id': None,
 'reasoning': None,
 'status': 'completed',
 'text': {'format': {'type': 'text'}},
 'truncation': 'auto',
 'usage': {'input_tokens': 328,
  'output_tokens': 57,
  'output_tokens_details': {'reasoning_tokens': 0},
  'total_tokens': 385,
  'input_tokens_details': {'cached_tokens': 0}},
 'user': '00000000-0000-0000-0000-000000000000',
 'store': True}
```
</Accordion>
</AccordionGroup>

<Tip>
Currently our Responses API only supports the following functionality:

- Text input
- Image input
- Function calling
- Reasoning
- Web search

We are working on adding support for more functionality in the future.
</Tip>

### 6.2. Retrieving a Response

<CodeGroup>
```python Python
from openai import OpenAI

openai_client = OpenAI(base_url="http://localhost:8080/api", api_key="YOUR_JULEP_API_KEY")
response = openai_client.responses.retrieve(
    response_id="response_id_here"
)
print("Retrieved response:", response.output[0].content[0].text)
```

```javascript Node.js
import { OpenAI } from 'openai';

const openai_client = new OpenAI({ baseURL: 'http://localhost:8080/api', apiKey: 'YOUR_JULEP_API_KEY' });
const response = await openai_client.responses.retrieve({
    response_id: "response_id_here"
});
console.log("Retrieved response:", response.output[0].content[0].text);
```
</CodeGroup>

Follow the example below to see a sample response from the Responses API.

<AccordionGroup>
<Accordion title="Sample Response"> 

```json JSON Response
{'id': '067dca81-8c8a-7f8a-8000-5fd0beb5ef60',
 'created_at': 1742514226.0,
 'error': None,
 'incomplete_details': None,
 'instructions': None,
 'metadata': {},
 'model': 'gpt-4o-mini',
 'object': 'response',
 'output': [{'id': '067dca83-2c61-74ff-8000-2ab58da8ec5f',
   'content': [{'annotations': [],
     'text': 'First, note that from 2 p.m. to 3 p.m., the train leaving Chicago travels for 1 hour at 60 mph, covering 60 miles. After this hour, the remaining distance between the two trains is 1000 âˆ’ 60 = 940 miles.\n\nStarting at 3 p.m., the two trains approach each other. Their combined speed is 60 + 75 = 135 mph. Let T be the number of hours after 3 p.m. until they meet. Then:\n\n135 Ã— T = 940  \nT = 940 Ã· 135  \nT â‰ˆ 6.96 hours\n\nThis is about 6 hours and 57 minutes, so starting from 3 p.m., they meet at approximately 9:57 p.m.',
     'type': 'output_text'}],
   'role': 'assistant',
   'status': 'completed',
   'type': 'message'}],
 'parallel_tool_calls': True,
 'temperature': 1.0,
 'tool_choice': 'auto',
 'tools': [],
 'top_p': 1.0,
 'max_output_tokens': None,
 'previous_response_id': None,
 'reasoning': None,
 'status': 'completed',
 'text': {'format': {'type': 'text'}},
 'truncation': 'disabled',
 'usage': {'input_tokens': 0,
  'output_tokens': 167,
  'output_tokens_details': {'reasoning_tokens': 0},
  'total_tokens': 167,
  'input_tokens_details': {'cached_tokens': 0}},
 'user': None,
 'store': True}
```
</Accordion>
</AccordionGroup>


<Tip>
Check out the [API reference](/api-reference/responses) or SDK reference ([Python](/sdks/python/reference#responses) or [JavaScript](/sdks/nodejs/reference#responses)) for more details on different operations you can perform with the Responses API.
</Tip>

## 7. Relationship to Sessions

While [Sessions](/docs/concepts/sessions) provide a persistent, stateful way to interact with agents over multiple turns, the Responses API offers a lightweight, stateless alternative for quick, one-off interactions with language models. Here's how they compare:

| Feature | Sessions | Responses |
|---------|----------|-----------|
| **State Management** | Maintains conversation history | Stateless (with optional context from previous responses) |
| **Persistence** | Long-lived, for ongoing conversations | Short-lived, for one-off interactions |
| **Agent Integration** | Requires an agent | No agent needed |
| **Setup Complexity** | Requires agent and session creation | Minimal setup (just model and input) |
| **Use Case** | Multi-turn conversations, complex interactions | Quick content generation, processing, or reasoning |

<Info>
If you need to maintain context across multiple interactions but prefer the simplicity of the Responses API, you can use the `previous_response_id` parameter to link responses together.
</Info>


## 8. Best Practices

<CardGroup cols={3}>
  <Card title="Optimize Input Prompts" icon="lightbulb">
    <ul>
      <li>**1. Be Specific**: Clearly define what you want the model to generate.</li>
      <li>**2. Provide Context**: Include relevant background information in your prompt.</li>
      <li>**3. Use Examples**: When appropriate, include examples of desired outputs in your prompt.</li>
    </ul>
  </Card>

  <Card title="Model Selection" icon="robot">
    <ul>
      <li>**1. Match Complexity**: Use more capable models for complex tasks (e.g., reasoning, coding).</li>
      <li>**2. Consider Latency**: Smaller models are faster for simple tasks.</li>
      <li>**3. Test Different Models**: Compare results across models for optimal performance.</li>
    </ul>
  </Card>

  <Card title="Tool Usage" icon="screwdriver-wrench">
    <ul>
      <li>**1. Provide Clear Tool Descriptions**: Help the model understand when and how to use tools.</li>
      <li>**2. Only Include Relevant Tools**: Too many tools can confuse the model's selection process.</li>
      <li>**3. Validate Tool Outputs**: Always verify the information returned from tool calls.</li>
    </ul>
  </Card>
</CardGroup>

## 9. Next Steps

- [Tool Integration](/docs/concepts/tools) - Learn about tools and how to use them with responses
- [Sessions](/docs/concepts/sessions) - Explore stateful conversations when you need ongoing context
- [Model Selection](/docs/integrations/supported-models) - Learn about available language models in Julep