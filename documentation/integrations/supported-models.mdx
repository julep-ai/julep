---
title: 'Supported Models'
description: 'Comprehensive guide to AI models and parameters supported by Julep'
icon: 'bars'
---

## Overview

Julep leverages LiteLLM to seamlessly connect you to a wide array of Language Models (LLMs). This integration offers incredible flexibility, allowing you to tap into models from various providers with a straightforward, unified interface.

<Info>
With our unified API, switching between different providers is a breeze, ensuring you maintain consistent functionality across the board.
</Info>

## Available Models

<Info>
While we provide API keys for quick testing and development, you'll need to use your own API keys when deploying to production. This ensures you have full control over your usage and billing.
</Info>

<Tip>
Looking for top-notch quality? Our curated selection of models delivers excellent outputs for all your use cases.
</Tip>

### Anthropic

Here are the Anthropic models supported by Julep:

| Model Name | Context Window | Best For |
|------------|---------------|-----------|
| claude-3-opus | 200K tokens | Complex reasoning, analysis |
| claude-3-sonnet | 200K tokens | General purpose tasks |
| claude-3-haiku | 200K tokens | Quick responses |
| claude-3.5-haiku | 200K tokens | Improved reasoning |
| claude-3.5-sonnet | 200K tokens | Improved reasoning |
| claude-3.5-sonnet-20240620 | 200K tokens | Enhanced reasoning capabilities |
| claude-3.5-sonnet-20241022 | 200K tokens | Computer Use Capabilities and one of the latest models |
| claude-3.7-sonnet | 200K tokens | Reasoning abilities and the latest model from Anthropic |
| claude-4-opus | 200K tokens | Most advanced reasoning and analysis capabilities |
| claude-4-sonnet | 200K tokens | Balanced performance with latest improvements |

### Google

Here are the Google models supported by Julep:

| Model Name | Context Window | Best For |
|------------|---------------|-----------|
| gemini-1.5-pro | 1M tokens | Complex tasks |
| gemini-1.5-pro-latest | 1M tokens | Cutting-edge performance |
| gemini-2.0-flash | 1M tokens | Next generation features, speed, thinking, realtime streaming, and multimodal generation |
| gemini-2.5-pro-preview-03-25 | 1M tokens | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more |
| gemini-2.5-pro-preview-06-05 | 1M tokens | Latest preview with improved capabilities |

### OpenAI

Here are the OpenAI models supported by Julep:

| Model Name | Context Window | Best For |
|------------|---------------|-----------|
| gpt-4-turbo | 128K tokens | Advanced reasoning |
| gpt-4o-mini | 128K tokens | Balanced performance |
| gpt-4o | 128K tokens | Balanced performance |
| gpt-4.1 | 128K tokens | Latest generation with improved performance |
| gpt-4.1-mini | 128K tokens | Efficient version of GPT-4.1 |
| gpt-4.1-nano | 128K tokens | Lightweight version for quick tasks |
| o1-mini | 200K tokens | Quick tasks |
| o1-preview | 200K tokens | Testing features |
| o1 | 200K tokens | General tasks |
| o3-mini | 200K tokens | Suited for reasoning tasks |
| o4-mini | 200K tokens | Next generation reasoning model |

### Groq

Here are the Groq models supported by Julep:

| Model Name | Context Window | Best For |
|------------|---------------|-----------|
| llama-3.1-70b | 8K tokens | Long-form content |
| llama-3.1-8b | 8K tokens | Quick processing |
| llama-3.1-8b-instant | 8K tokens | Fastest processing for simple tasks |
| llama-3.3-70b-versatile | 8K tokens | Versatile tasks with improved capabilities |
| gemma2-9b-it | 8K tokens | Instruction-tuned responses |
| deepseek-r1-distill-llama-70b | 8K tokens | Advanced reasoning with DeepSeek distillation |
| meta-llama/llama-4-maverick-17b-128e-instruct | 128K tokens | Extended context advanced reasoning |
| meta-llama/llama-4-scout-17b-16e-instruct | 16K tokens | Efficient scouting and analysis |
| meta-llama/Llama-Guard-4-12B | 8K tokens | Content moderation and safety |
| mistral-saba-24b | 8K tokens | Balanced performance and efficiency |
| qwen-qwq-32b | 32K tokens | Question answering and reasoning |
| qwen/qwen3-32b | 32K tokens | Multilingual support and reasoning |

### OpenRouter

Here are the OpenRouter models supported by Julep:

| Model Name | Context Window | Best For |
|------------|---------------|-----------|
| mistral-large-2411 | 128K tokens | High performance |
| qwen-2.5-72b-instruct | 131K tokens | Complex instructions |
| eva-llama-3.33-70b | 128K tokens | Story writing and creative fiction |
| l3.1-euryale-70b | 128K tokens | Poetry and artistic writing |
| l3.3-euryale-70b | 128K tokens | Advanced creative writing and roleplay |
| magnum-v4-72b | 8K tokens | Content generation and brainstorming |
| eva-qwen-2.5-72b | 8K tokens | Creative problem solving and ideation |
| hermes-3-llama-3.1-70b | 8K tokens | Narrative design and worldbuilding |
| deepseek-chat | 32K tokens | Conversational AI |
| meta-llama/llama-4-scout | 10M tokens | General purpose tasks, long-context processing, code analysis |
| meta-llama/llama-4-scout:free | 10M tokens | Free tier usage, same capabilities as llama-4-scout |
| meta-llama/llama-4-maverick | 10M tokens | Advanced reasoning, coding, multilingual tasks, image understanding |
| meta-llama/llama-4-maverick:free | 10M tokens | Free tier usage, same capabilities as llama-4-maverick |

### Cerebras

Here are the Cerebras models supported by Julep:

| Model Name | Context Window | Best For |
|------------|---------------|-----------|
| cerebras/llama-3.1-8b | 8K tokens | Quick creative writing and basic text generation |
| cerebras/llama-3.3-70b | 8K tokens | Complex creative writing, storytelling, and detailed content generation |
| cerebras/llama-4-scout-17b-16e-instruct | 10M tokens | General purpose tasks, long-context processing, code analysis (Much faster than other deployments at over 2,600 token/sec) |
| cerebras/deepseek-r1-distill-llama-70b | 8K tokens | High-speed reasoning with DeepSeek distillation |
| cerebras/qwen-3-32b | 32K tokens | Fast multilingual processing |

### Amazon Nova

Here are the Amazon Nova models supported by Julep (via OpenRouter):

| Model Name | Context Window | Best For |
|------------|---------------|-----------|
| amazon/nova-lite-v1 | 300K tokens | Lightweight tasks with large context |
| amazon/nova-micro-v1 | 128K tokens | Efficient processing for simple tasks |
| amazon/nova-pro-v1 | 300K tokens | Professional-grade capabilities |

### Embedding

Here are the embedding models supported by Julep:

| Model Name | Embedding Dimensions | Best For |
|------------|---------------|-----------|
| text-embedding-3-large | 1024 | High-quality vectors |
| voyage-multilingual-2 | 1024 | Cross-language tasks |
| voyage-3 | 1024 | Advanced embeddings |
| Alibaba-NLP/gte-large-en-v1.5 | 1024 | Cost-effective solutions |
| BAAI/bge-m3 | 1024 | Cost-effective solutions |
| vertex_ai/text-embedding-004 | 1024 | Google Cloud integration |

<Info>
Though the models mention above support different embedding dimensions, Julep uses  fixed 1024 dimensions for all embedding models for now. We plan to support different dimensions in the future.
</Info>

## Supported Parameters

Following are a list of different parameters that can be used to control the behavior of the models.

<AccordionGroup>
  <Accordion title="Core Parameters" icon="sliders" defaultOpen={true}>
    | Parameter | Range | Description |
    |-----------|--------|-------------|
    | temperature | 0.0 - 5.0 | Controls randomness in outputs. Higher values (e.g., 0.8) increase randomness, while lower values (e.g., 0.2) make output more focused and deterministic |
    | top_p | 0.0 - 1.0 | Alternative to temperature for nucleus sampling. Only tokens with cumulative probability < top_p are considered. We recommend adjusting either this or temperature, not both |
    | max_tokens | â‰¥ 1 | Maximum number of tokens to generate in the response |
  </Accordion>

  <Accordion title="Penalty Parameters" icon="gauge" defaultOpen={true}>
    | Parameter | Range | Description |
    |-----------|--------|-------------|
    | frequency_penalty | -2.0 - 2.0 | Penalizes tokens based on their frequency in the text. Positive values decrease repetition |
    | presence_penalty | -2.0 - 2.0 | Penalizes tokens based on their presence in the text. Positive values decrease likelihood of repeating content |
    | repetition_penalty | 0.0 - 2.0 | Penalizes repetition (1.0 is neutral). Values > 1.0 reduce likelihood of repeating content |
    | length_penalty | 0.0 - 2.0 | Penalizes based on generation length (1.0 is neutral). Values > 1.0 penalize longer generations |
  </Accordion>

  <Accordion title="Advanced Controls" icon="gear" defaultOpen={true}>
    | Parameter | Range | Description |
    |-----------|--------|-------------|
    | min_p | 0.0 - 1.0 | Minimum probability threshold compared to the highest token probability |
    | seed | integer | For deterministic generation. Set a specific seed for reproducible results |
    | stop | list[str] | Up to 4 sequences where generation should stop |
  </Accordion>

</AccordionGroup>
<Info>
Not all parameters are supported by every model. Please refer to the [LiteLLM documentation](https://docs.litellm.ai/completion/input) for more details.
</Info>

<Note>
  **Best Practices:**
  - Start with default values and adjust based on your needs
  - Use temperature (0.0 - 1.0) for most cases
  - Avoid setting multiple penalty parameters simultaneously
  - Test different combinations for optimal results
</Note>

<Warning>
  Setting extreme values for multiple parameters may lead to unexpected behavior or poor quality outputs.
</Warning>

## Usage Guidelines

<CardGroup cols={2}>
  <Card title="Consider Model Selection Criteria" icon="chart-line">
    <ul>
      <li>**1.** Your budget and cost constraints</li>
      <li>**2.** How fast you need responses</li>
      <li>**3.** The quality you're aiming for</li>
      <li>**4.** The context window size you require</li>
    </ul>
  </Card>

  <Card title="Follow Best Practices" icon="check">
    <ul>
      <li>**1.** Start with smaller models for development and testing</li>
      <li>**2.** Use larger context windows only when necessary</li>
      <li>**3.** Keep an eye on token usage to manage costs</li>
    </ul>
  </Card>
</CardGroup>
  
<Tip>
  For more information, please refer to the [LiteLLM documentation](https://docs.litellm.ai/providers).
</Tip>